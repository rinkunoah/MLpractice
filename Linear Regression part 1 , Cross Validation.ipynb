{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be working with a data set based on [housing prices in Ames, Iowa](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). It was compiled for educational use to be a modernized and expanded alternative to the well-known Boston Housing dataset. This version of the data set has had some missing values filled for convenience.\n",
    "\n",
    "There are an extensive number of features, so they've been described in the table below.\n",
    "\n",
    "### Predictor\n",
    "\n",
    "* SalePrice: The property's sale price in dollars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Import the data using the file path\n",
    "data_path = ['C:\\IntelPython3\\projects']\n",
    "filepath = os.sep.join(data_path + ['Ames_Housing_Sales.csv'])\n",
    "data = pd.read_csv(filepath, sep=',')\n",
    "data['Alley'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "As discussed in the lecture, a significant challenge, particularly when dealing with data that have many columns, is ensuring each column gets encoded correctly. \n",
    "\n",
    "This is particularly true with data columns that are ordered categoricals (ordinals) vs unordered categoricals. Unordered categoricals should be one-hot encoded, however this can significantly increase the number of features and creates features that are highly correlated with each other.\n",
    "\n",
    "Determine how many total features would be present, relative to what currently exists, if all string (object) features are one-hot encoded. Recall that the total number of one-hot encoded columns is `n-1`, where `n` is the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=data.select_dtypes(include=[object])\n",
    "l=d[d.columns].apply(lambda x:x.nunique())\n",
    "l=l.loc[l>1] # values which contains more than 1 value\n",
    "l-=1\n",
    "\n",
    "# relative total features\n",
    "l.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Just for understanding clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=pd.get_dummies(d)\n",
    "p=pd.DataFrame(p)\n",
    "count=0\n",
    "for i in p.nunique():\n",
    "    if i>1:\n",
    "        count+=1\n",
    "count-d.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Let's create a new data set where all of the above categorical features will be one-hot encoded. We can fit this data and see how it affects the results.\n",
    "\n",
    "* Used the dataframe `.copy()` method to create a completely separate copy of the dataframe for one-hot encoding\n",
    "* On this new dataframe, one-hot encode each of the appropriate columns and add it back to the dataframe. Be sure to drop the original column.\n",
    "* For the data that are not one-hot encoded, drop the columns that are string categoricals.\n",
    "\n",
    "For the first step, numerically encoding the string categoricals, either Scikit-learn;s `LabelEncoder` or `DictVectorizer` can be used. However, the former is probably easier since it doesn't require specifying a numerical value for each category, and we are going to one-hot encode all of the numerical values anyway. (Can you think of a time when `DictVectorizer` might be preferred?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe=data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_3</th>\n",
       "      <th>SaleType_4</th>\n",
       "      <th>SaleType_5</th>\n",
       "      <th>SaleType_6</th>\n",
       "      <th>SaleType_7</th>\n",
       "      <th>SaleType_8</th>\n",
       "      <th>Street_0</th>\n",
       "      <th>Street_1</th>\n",
       "      <th>Utilities_0</th>\n",
       "      <th>Utilities_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856.0</td>\n",
       "      <td>854.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920.0</td>\n",
       "      <td>866.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145.0</td>\n",
       "      <td>1053.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>953.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>2073.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>790.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>1188.0</td>\n",
       "      <td>1152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>1078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1256.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>830.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1379 rows Ã— 295 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0        856.0     854.0        0.0             3       706.0         0.0   \n",
       "1       1262.0       0.0        0.0             3       978.0         0.0   \n",
       "2        920.0     866.0        0.0             3       486.0         0.0   \n",
       "3        961.0     756.0        0.0             3       216.0         0.0   \n",
       "4       1145.0    1053.0        0.0             4       655.0         0.0   \n",
       "...        ...       ...        ...           ...         ...         ...   \n",
       "1374     953.0     694.0        0.0             3         0.0         0.0   \n",
       "1375    2073.0       0.0        0.0             3       790.0       163.0   \n",
       "1376    1188.0    1152.0        0.0             4       275.0         0.0   \n",
       "1377    1078.0       0.0        0.0             2        49.0      1029.0   \n",
       "1378    1256.0       0.0        0.0             3       830.0       290.0   \n",
       "\n",
       "      BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  SaleType_3  \\\n",
       "0                1             0      150.0            0.0  ...         0.0   \n",
       "1                0             1      284.0            0.0  ...         0.0   \n",
       "2                1             0      434.0            0.0  ...         0.0   \n",
       "3                1             0      540.0          272.0  ...         0.0   \n",
       "4                1             0      490.0            0.0  ...         0.0   \n",
       "...            ...           ...        ...            ...  ...         ...   \n",
       "1374             0             0      953.0            0.0  ...         0.0   \n",
       "1375             1             0      589.0            0.0  ...         0.0   \n",
       "1376             0             0      877.0            0.0  ...         0.0   \n",
       "1377             1             0        0.0          112.0  ...         0.0   \n",
       "1378             1             0      136.0            0.0  ...         0.0   \n",
       "\n",
       "      SaleType_4  SaleType_5  SaleType_6  SaleType_7  SaleType_8  Street_0  \\\n",
       "0            0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "1            0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "2            0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "3            0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "4            0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "...          ...         ...         ...         ...         ...       ...   \n",
       "1374         0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "1375         0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "1376         0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "1377         0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "1378         0.0         0.0         0.0         0.0         1.0       0.0   \n",
       "\n",
       "      Street_1  Utilities_0  Utilities_1  \n",
       "0          1.0          1.0          0.0  \n",
       "1          1.0          1.0          0.0  \n",
       "2          1.0          1.0          0.0  \n",
       "3          1.0          1.0          0.0  \n",
       "4          1.0          1.0          0.0  \n",
       "...        ...          ...          ...  \n",
       "1374       1.0          1.0          0.0  \n",
       "1375       1.0          1.0          0.0  \n",
       "1376       1.0          1.0          0.0  \n",
       "1377       1.0          1.0          0.0  \n",
       "1378       1.0          1.0          0.0  \n",
       "\n",
       "[1379 rows x 295 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "lenc = preprocessing.LabelEncoder()\n",
    "onehc=OneHotEncoder()\n",
    "l=onehc.fit_transform([[4],[1],[0]])\n",
    "dataf=data.drop(columns=d.columns)\n",
    "for i in d.columns:\n",
    "    temp_series=lenc.fit_transform(new_dataframe[i])\n",
    "    col_names=[i+'_'+str(j) for j in list(set(temp_series))]\n",
    "    onecode=onehc.fit_transform(temp_series.reshape(-1,1))\n",
    "    new_Data=pd.DataFrame(onecode.toarray(),columns=col_names)\n",
    "    \n",
    "    dataf = pd.concat([dataf,new_Data], axis=1)\n",
    "f=l.toarray()\n",
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=d.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Create train and test splits of both data sets. To ensure the data gets split the same way, use the same `random_state` in each of the two splits.\n",
    "* For each data set, fit a basic linear regression model on the training data. \n",
    "* Calculate the mean squared error on both the train and test sets for the respective models. Which model produces smaller error on the test data and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is train and test splits of non encoded data i.e dataf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# non encoded\n",
    "\n",
    "X1=[i for i in data.columns if i!='SalePrice']\n",
    "X_data1=data[X1]\n",
    "Y_data1=data['SalePrice']\n",
    "X_train1,X_test1,Y_train1,Y_test1 = train_test_split(X_data1,Y_data1, test_size=0.3,random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# one hot encoded\n",
    "X2=[i for i in dataf.columns if i!='SalePrice']\n",
    "X_data2=dataf[X2]\n",
    "Y_data2=dataf['SalePrice']\n",
    "X_train2,X_test2,Y_train2,Y_test2 = train_test_split(X_data2,Y_data2, test_size=0.3,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR= LinearRegression()\n",
    "LR= LR.fit(X_train1, Y_train1)\n",
    "Y_predict= LR.predict(X_test1)\n",
    "Y_predictwithtrain=LR.predict(X_train1)\n",
    "\n",
    "Y_testmeanerror=mean_squared_error(Y_test1,Y_predict)\n",
    "Y_trainmeanerror=mean_squared_error(Y_train1,Y_predictwithtrain)\n",
    "min(Y_testmeanerror,Y_trainmeanerror)\n",
    "Y_testmeanerror,Y_trainmeanerror\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is train and test splits for onehotencoded data i.e data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR=LR.fit(X_train2,Y_train2)\n",
    "Y_predict2=LR.predict(X_test2)\n",
    "Y_predict1=LR.predict(X_train2)\n",
    "Y_predict2,Y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testmeanerror2=mean_squared_error(Y_test2,Y_predict2)\n",
    "Y_trainmeanerror1=mean_squared_error(Y_train2,Y_predict1)\n",
    "Y_testmeanerror2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "* mean squared error for one hot encoding data is smaller than mean squared error for non encoded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5\n",
    "\n",
    "For each of the data sets (one-hot encoded and not encoded):\n",
    "\n",
    "* Scale the all the non-hot encoded values using one of the following: `StandardScaler`, `MinMaxScaler`, `MaxAbsScaler`.\n",
    "* Compare the error calculated on the test sets\n",
    "\n",
    "Be sure to calculate the skew (to decide if a transformation should be done) and fit the scaler on *ONLY* the training data, but then apply it to both the train and test data identically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For non encoded data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sd=StandardScaler()\n",
    "minmax=MinMaxScaler()\n",
    "maxabs=MaxAbsScaler()\n",
    "def scale(xtrain,xtest,ytrain,ytest,scaler):\n",
    "\n",
    "    \n",
    "    x1=scaler.fit_transform(xtrain)\n",
    "    LR = LinearRegression()\n",
    "    LR=LR.fit(x1,ytrain)\n",
    "    Y_predict2=LR.predict(xtest)\n",
    "    ytestmeanerror=mean_squared_error(ytest,Y_predict2)\n",
    "    return ytestmeanerror\n",
    "# X=data.select_dtypes(include=[float])\n",
    "# p=[i for i in X.columns if i!='SalePrice']\n",
    "# #non encoded\n",
    "# Xdata=X[p]\n",
    "# Ydata=X['SalePrice']\n",
    "# X_train1,X_test1,Y_train1,Y_test1 = train_test_split(Xdata,Ydata, test_size=0.3,random_state=42)\n",
    "\n",
    "\n",
    "print(scale(X_train1,X_test1,Y_train1,Y_test1,sd))\n",
    "print(scale(X_train1,X_test1,Y_train1,Y_test1,minmax))\n",
    "print(scale(X_train1,X_test1,Y_train1,Y_test1,maxabs))\n",
    "print(scale(X_train2,X_test2,Y_train2,Y_test2,sd))\n",
    "print(scale(X_train2,X_test2,Y_train2,Y_test2,minmax))\n",
    "print(scale(X_train2,X_test2,Y_train2,Y_test2,maxabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 6\n",
    "* Plot predictions vs actual for one of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot for one hot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('talk')\n",
    "sns.set_style('ticks')\n",
    "sns.set_palette('dark')\n",
    "a=plt.axes()\n",
    "a.scatter(Y_test2,Y_predict2,alpha=.5)\n",
    "a.set(xlabel='Ytest data',ylabel='predict',title='One hot-encoded plot')\n",
    "Y_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot for non encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p=plt.axes()\n",
    "sns.set_context('talk')\n",
    "sns.set_style('ticks')\n",
    "sns.set_palette('dark')\n",
    "p.scatter(Y_test1,Y_predict,alpha=.5)\n",
    "p.set(xlabel='Ytest data',ylabel='predict',title='Non-encoded plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
